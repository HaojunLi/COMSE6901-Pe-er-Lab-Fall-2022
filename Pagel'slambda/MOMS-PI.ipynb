{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fall 2022 Pagel's lambda\n",
    "from os import name\n",
    "from biom import load_table\n",
    "from re import L\n",
    "import numpy as np\n",
    "import math\n",
    "from numpy.core.fromnumeric import ptp\n",
    "from numpy.linalg import inv, det, pinv, slogdet\n",
    "import pandas as pd\n",
    "from ete3 import Tree\n",
    "from Bio import Phylo\n",
    "from six import b\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import biom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Brownian motion likelihood computation\n",
    "# Brownian motion models can be completely described by two parameters. The first is the starting value of the population mean trait, z¯(0).\n",
    "# This is the mean trait value that is seen in the ancestral population at the start of the simulation, before any trait change occurs.\n",
    "# The second parameter of Brownian motion is the evolutionary rate parameter, σ^2. This parameter determines how fast traits will randomly walk through time.\n",
    "\n",
    "# Under Brownian motion, changes in trait values over any interval of time are always drawn from a normal distribution\n",
    "# with mean 0 and variance proportional to the product of the rate of evolution and the length of time (variance = σ^2t).\n",
    "# x is an n x 1 vector of trait values for the n tip species in the tree\n",
    "\n",
    "np.seterr(over='raise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Z0_delatSquare_basic_Unittest(val, text):\n",
    "    randomMatrix = np.array([[1]])\n",
    "    randomNumpyInt = np.array([1])[0]\n",
    "    randomNumpyFloat = np.array([11.1])[0]\n",
    "    if type(val) != type(randomMatrix):\n",
    "        raise TypeError(f\"{text} should be a numpy array\")\n",
    "    if len(val.shape) != 2:\n",
    "        raise ValueError(f\"{text} should be np.array([[value]])\")\n",
    "    if val.shape[0] != 1 or val.shape[1] != 1:\n",
    "        raise ValueError(f\"{text} should be np.array([[value]])\")\n",
    "    if type(val[0][0]) != type(randomNumpyInt) and type(val[0][0]) != type (randomNumpyFloat):\n",
    "        raise TypeError(f\"Value in {text} sould be either {type(randomNumpyInt)} or {type(randomNumpyFloat)}\")\n",
    "\n",
    "# x is an n x 1 vector of trait\n",
    "def Basic_unittest(X,Z0, deltaSquare, C):\n",
    "    randomMatrix = np.array([[1]])\n",
    "    randomNumpyInt = np.array([1])[0]\n",
    "    randomNumpyFloat = np.array([11.1])[0]\n",
    "    # X OK\n",
    "    if type(X) != type(randomMatrix):\n",
    "        raise TypeError(\"X should be a numpy array\")\n",
    "    if len(X.shape) != 2:\n",
    "        raise ValueError(\"X should be an n x 1 vector\")\n",
    "    if X.shape[1] != 1:\n",
    "        raise ValueError(\"X should be an n x 1 vector\")\n",
    "    for x in X:\n",
    "        j = x[0]\n",
    "        if type(j) != type(randomNumpyInt) and type(j) != type(randomNumpyFloat):\n",
    "            raise TypeError(f\"Value in X sould be either {type(randomNumpyInt)} or {type(randomNumpyFloat)}\")\n",
    "    # Z0 OK\n",
    "    Z0_delatSquare_basic_Unittest(Z0, \"Z0\")\n",
    "    \n",
    "    # deltaSquare OK\n",
    "    Z0_delatSquare_basic_Unittest(deltaSquare, \"deltaSquare\")\n",
    "    if deltaSquare[0][0] < 0:\n",
    "        raise ValueError(\"deltaSquare should be greater than or equal to 0\")\n",
    "        \n",
    "    # C OK\n",
    "    if type(C) != type(randomMatrix):\n",
    "        raise TypeError(\"C should be a numpy array\")\n",
    "    if len(C.shape) != 2:\n",
    "        raise ValueError(\"C should be an n x n symmetric matrix\")\n",
    "    if C.shape[0] != C.shape[1]:\n",
    "        raise ValueError(\"C should be an n x n symmetric matrix\")\n",
    "    if not np.array_equal(C, C.T):\n",
    "        raise ValueError(\"C should be an n x n symmetric matrix\")\n",
    "    for c in C:\n",
    "        for j in c:\n",
    "            if type(j) != type(randomNumpyInt) and type(j) != type(randomNumpyFloat):\n",
    "                raise TypeError(f\"Value in C sould be either {type(randomNumpyInt)} or {type(randomNumpyFloat)}\")\n",
    "    if X.shape[0] != C.shape[0]:\n",
    "        raise ValueError(\"X should be an n x 1 vector and C should be an n x n symmetric matrix\")\n",
    "# Finished\n",
    "\n",
    "def Basic_unittest_X_C(X, C):\n",
    "    randomMatrix = np.array([[1]])\n",
    "    randomNumpyInt = np.array([1])[0]\n",
    "    randomNumpyFloat = np.array([11.1])[0]\n",
    "    # X OK\n",
    "    if type(X) != type(randomMatrix):\n",
    "        raise TypeError(\"X should be a numpy array\")\n",
    "    if len(X.shape) != 2:\n",
    "        raise ValueError(\"X should be an n x 1 vector\")\n",
    "    if X.shape[1] != 1:\n",
    "        raise ValueError(\"X should be an n x 1 vector\")\n",
    "    for x in X:\n",
    "        j = x[0]\n",
    "        if type(j) != type(randomNumpyInt) and type(j) != type(randomNumpyFloat):\n",
    "            raise TypeError(f\"Value in X sould be either {type(randomNumpyInt)} or {type(randomNumpyFloat)}\")        \n",
    "    # C OK\n",
    "    if type(C) != type(randomMatrix):\n",
    "        raise TypeError(\"C should be a numpy array\")\n",
    "    if len(C.shape) != 2:\n",
    "        raise ValueError(\"C should be an n x n symmetric matrix\")\n",
    "    if C.shape[0] != C.shape[1]:\n",
    "        raise ValueError(\"C should be an n x n symmetric matrix\")\n",
    "    if not np.array_equal(C, C.T):\n",
    "        raise ValueError(\"C should be an n x n symmetric matrix\")\n",
    "    for c in C:\n",
    "        for j in c:\n",
    "            if type(j) != type(randomNumpyInt) and type(j) != type(randomNumpyFloat):\n",
    "                raise TypeError(f\"Value in C sould be either {type(randomNumpyInt)} or {type(randomNumpyFloat)}\")\n",
    "    if X.shape[0] != C.shape[0]:\n",
    "        raise ValueError(\"X should be an n x 1 vector and C should be an n x n symmetric matrix\")\n",
    "# Finished\n",
    "\n",
    "def Basic_unittest_C(C):\n",
    "    randomMatrix = np.array([[1]])\n",
    "    randomNumpyInt = np.array([1])[0]\n",
    "    randomNumpyFloat = np.array([11.1])[0]    \n",
    "    # C OK\n",
    "    if type(C) != type(randomMatrix):\n",
    "        raise TypeError(\"C should be a numpy array\")\n",
    "    if len(C.shape) != 2:\n",
    "        raise ValueError(\"C should be an n x n symmetric matrix\")\n",
    "    if C.shape[0] != C.shape[1]:\n",
    "        raise ValueError(\"C should be an n x n symmetric matrix\")\n",
    "    if not np.array_equal(C, C.T):\n",
    "        raise ValueError(\"C should be an n x n symmetric matrix\")\n",
    "    for c in C:\n",
    "        for j in c:\n",
    "            if type(j) != type(randomNumpyInt) and type(j) != type(randomNumpyFloat):\n",
    "                raise TypeError(f\"Value in C sould be either {type(randomNumpyInt)} or {type(randomNumpyFloat)}\")\n",
    "# Finished\n",
    "\n",
    "def Basic_unittest_X(X):\n",
    "    randomMatrix = np.array([[1]])\n",
    "    randomNumpyInt = np.array([1])[0]\n",
    "    randomNumpyFloat = np.array([11.1])[0]\n",
    "    # X OK\n",
    "    if type(X) != type(randomMatrix):\n",
    "        raise TypeError(\"X should be a numpy array\")\n",
    "    if len(X.shape) != 2:\n",
    "        raise ValueError(\"X should be an n x 1 vector\")\n",
    "    if X.shape[1] != 1:\n",
    "        raise ValueError(\"X should be an n x 1 vector\")\n",
    "    for x in X:\n",
    "        j = x[0]\n",
    "        if type(j) != type(randomNumpyInt) and type(j) != type(randomNumpyFloat):\n",
    "            raise TypeError(f\"Value in X sould be either {type(randomNumpyInt)} or {type(randomNumpyFloat)}\")\n",
    "# Finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''We can calculate the likelihood of obtaining\n",
    "the data under our Brownian motion model using a standard formula for the\n",
    "likelihood of drawing from a multivariate normal distribution (Harmon & Open Textbook Library, 2019)'''\n",
    "# Reference is:\n",
    "# Harmon, L. J. & Open Textbook Library. (2019). Phylogenetic Comparative Methods. https://open.umn.edu/opentextbooks/textbooks/691 \n",
    "# Section 4.3: Estimating rates using maximum likelihood (eq. 4.5)\n",
    "# Since it is an implementation of a mathematic formular, please compare our implementation with that mathematic formular in the reference.\n",
    "def Brownian_motion_likelihood(X,Z0, deltaSquare, C):\n",
    "    Basic_unittest(X,Z0, deltaSquare, C)\n",
    "    X = X * 1.0\n",
    "    Z0 = Z0 * 1.0\n",
    "    deltaSquare = deltaSquare * 1.0\n",
    "    C = C * 1.0\n",
    "    # one is n x 1 vector of 1\n",
    "    one = np.full((len(X), 1), 1)\n",
    "    Z0_vector = Z0 * one\n",
    "    XSubZ0_vector = X - Z0_vector\n",
    "    # This is because pinv returns the inverse of your matrix when it is available and the pseudo inverse when it isn't.\n",
    "    temp = np.dot(np.dot(np.transpose(XSubZ0_vector), pinv(deltaSquare * C)), XSubZ0_vector)\n",
    "    numerator = math.exp(-1/2 * temp) # This is correct\n",
    "    try:\n",
    "        denominator = math.sqrt(((2 * math.pi) ** len(X)) * det(deltaSquare * C)) # This is correct\n",
    "        #denominator = math.sqrt(((2 * math.pi) ** len(X)) * deltaSquare**len(C) * det(C))\n",
    "    except FloatingPointError: # OK to have this error and continue executing next time\n",
    "        return 0\n",
    "    likelihood = numerator / denominator\n",
    "    return likelihood # correct\n",
    "\n",
    "# ln of Brownian motion likelihood\n",
    "def Ln_Brownian_motion_likelihood(X,Z0, deltaSquare, C):\n",
    "    Basic_unittest(X,Z0, deltaSquare, C)\n",
    "    X = X * 1.0\n",
    "    Z0 = Z0 * 1.0\n",
    "    deltaSquare = deltaSquare * 1.0\n",
    "    C = C * 1.0\n",
    "    # one is n x 1 vector of 1\n",
    "    one = np.full((len(X), 1), 1)\n",
    "    Z0_vector = Z0 * one\n",
    "    XSubZ0_vector = X - Z0_vector\n",
    "    # This is because pinv returns the inverse of your matrix when it is available and the pseudo inverse when it isn't.\n",
    "    temp = np.dot(np.dot(np.transpose(XSubZ0_vector), pinv(deltaSquare * C)), XSubZ0_vector)\n",
    "    front = -1/2 * temp\n",
    "    end = 1/2 * (len(X)*np.log(2 * math.pi) + slogdet(deltaSquare * C)[1])\n",
    "    return (front - end)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''In this case, the maximum-likelihood estimate for each of these two parameters can be calculated analytically (Harmon & Open Textbook Library, 2019)'''\n",
    "# Reference is:\n",
    "# Harmon, L. J. & Open Textbook Library. (2019). Phylogenetic Comparative Methods. https://open.umn.edu/opentextbooks/textbooks/691 \n",
    "# Section 4.3: Estimating rates using maximum likelihood (eq. 4.7) and (eq. 4.8)\n",
    "# Since it is an implementation of mathematic formulars, please compare our implementation with mathematic formulars in the reference.\n",
    "def Brownian_motion_maximumlikelihood(X, C):\n",
    "    Basic_unittest_X_C(X, C)\n",
    "    X = X * 1.0\n",
    "    C = C * 1.0\n",
    "    # one is n x 1 vector of 1\n",
    "    one = np.full((len(X), 1), 1)\n",
    "    z0hat_front = pinv(one.T @ pinv(C) @ one)\n",
    "    z0hat_end = one.T @ pinv(C) @ X\n",
    "    # estimated root state for the character\n",
    "    z0hat = z0hat_front * z0hat_end\n",
    "\n",
    "    # maximum likelihood delta square\n",
    "    numerator = (X - z0hat * one).T @ pinv(C) @ (X - z0hat * one)\n",
    "    denominator = len(X)\n",
    "    # estimated net rate of evolution\n",
    "    deltaSquarehat = numerator / denominator\n",
    "    # print(deltaSquarehat)\n",
    "    return z0hat, deltaSquarehat\n",
    "\n",
    "# Based on Pagel's lambda to transform the phylogenetic variance-covariance matrix.\n",
    "# compresses internal branches while leaving the tip branches of the tree unaffected\n",
    "# Reference is:\n",
    "# Harmon, L. J. & Open Textbook Library. (2019). Phylogenetic Comparative Methods. https://open.umn.edu/opentextbooks/textbooks/691 \n",
    "# Section 6.2: Transforming the evolutionary variancecovariance matrix (Equation 6.2)\n",
    "# Since it is an implementation of a mathematic formular, please compare our implementation with that mathematic formular in the reference.\n",
    "def lambdaCovarience(C, lambdaVal):\n",
    "    Basic_unittest_C(C)\n",
    "    if type(lambdaVal) != type(1) and type(lambdaVal) != type(0.5):\n",
    "        raise TypeError(\"lambdaVal should be either an integer or a float\")\n",
    "    # 0 <= lambda <= 1 \n",
    "    if (lambdaVal < 0) or (lambdaVal > 1):\n",
    "        raise ValueError(\"Lambda value: 0 <= lambda <= 1\")\n",
    "    C = C * 1.0\n",
    "    n = len(C)\n",
    "    for i in range(0,n):\n",
    "        for j in range(0,n):\n",
    "            # Off diagonal times lambda\n",
    "            if i != j:\n",
    "                C[i][j] = C[i][j] * lambdaVal\n",
    "    return C\n",
    "\n",
    "# Compute MLE for a given lambda value\n",
    "def Pagel_lambda_MLE(X, C, lambdaVal):\n",
    "    Basic_unittest_X_C(X, C)\n",
    "    if type(lambdaVal) != type(1) and type(lambdaVal) != type(0.5):\n",
    "        raise TypeError(\"lambdaVal should be either an integer or a float\")\n",
    "    # 0 <= lambda <= 1 \n",
    "    if (lambdaVal < 0) or (lambdaVal > 1):\n",
    "        raise ValueError(\"Lambda value: 0 <= lambda <= 1\")\n",
    "    X = X * 1.0\n",
    "    C = C * 1.0\n",
    "    # Compute new covarience matrix\n",
    "    C_lambda = lambdaCovarience(C, lambdaVal)\n",
    "    z0hat, deltaSquarehat = Brownian_motion_maximumlikelihood(X, C_lambda)\n",
    "    # Compute ln likelihood\n",
    "    Pagel_likelihood = Ln_Brownian_motion_likelihood(X,z0hat,deltaSquarehat, C_lambda)\n",
    "\n",
    "    return Pagel_likelihood, z0hat, deltaSquarehat, lambdaVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Searching with different step sizes. Finding out lambda's value to maximize likelihood\n",
    "def Found_Pagel_Maximumlikelihood(X, tree, stepSize, startSearch = 0, EndSearch = 1):\n",
    "    Basic_unittest_X(X)\n",
    "    # Just used to get type\n",
    "    defaultTree = Tree(\"(A:1,(B:1,(C:1,D:1):0.5):0.5);\")\n",
    "    if type(tree) != type(defaultTree):\n",
    "        raise TypeError(f\"tree type should be {type(defaultTree)}\")\n",
    "    treeLen = len(tree)\n",
    "    if X.shape[0] != treeLen:\n",
    "        raise ValueError(\"Wrong X and tree input, dimension does not match\")\n",
    "    # stepSize check\n",
    "    if type(stepSize) != type(1) and type(stepSize) != type(0.5):\n",
    "        raise TypeError(\"stepSize should be either an integer or a float\")\n",
    "    if (stepSize <= 0):\n",
    "        raise ValueError(\"stepSize value: 0 < stepSize\")\n",
    "    # startSearch check\n",
    "    if type(startSearch) != type(1) and type(startSearch) != type(0.5):\n",
    "        raise TypeError(\"startSearch should be either an integer or a float\")\n",
    "    if (startSearch < 0) or (startSearch > 1):\n",
    "        raise ValueError(\"startSearch value: 0 <= startSearch <= 1\")\n",
    "    # EndSearch check\n",
    "    if type(EndSearch) != type(1) and type(EndSearch) != type(0.5):\n",
    "        raise TypeError(\"EndSearch should be either an integer or a float\")\n",
    "    if (EndSearch < 0) or (EndSearch > 1):\n",
    "        raise ValueError(\"EndSearch value: 0 <= EndSearch <= 1\")\n",
    "    if startSearch > EndSearch:\n",
    "        raise ValueError(\"startSearch shoud be smaller than or equal to EndSearch\")\n",
    "    # OK\n",
    "    X = X * 1.0\n",
    "    '''One can in principle use some values of lambda greater than one on most variance-covariance\n",
    "        matrices, although many values of lambda > 1 result in matrices that are not valid\n",
    "        variance-covariance matrices and/or do not correspond with any phylogenetic\n",
    "        tree transformation. For this reason I recommend that lambda be limited to values\n",
    "        between 0 and 1.'''\n",
    "    # Initialization\n",
    "    lambdaVal = startSearch\n",
    "    maxlikelihood = -math.inf\n",
    "    maxlikelihood_lambda = -math.inf\n",
    "    # Record all likelihood and lambda value\n",
    "    likelihoodSave = []\n",
    "    lambdaValSave = []\n",
    "    # Try different lambda values and try to find its corresponding MLE\n",
    "    while lambdaVal <= EndSearch:\n",
    "        print(lambdaVal)\n",
    "        # Recompute C every time because it will be overwrited\n",
    "        C = Covariance(tree)\n",
    "        tmp_likelihood,tmp_z0hat, tmp_deltaSquarehat, tmp_lambdaVal= Pagel_lambda_MLE(X, C, lambdaVal)\n",
    "        # If tmp value is larger\n",
    "        if maxlikelihood < tmp_likelihood:\n",
    "            maxlikelihood = tmp_likelihood\n",
    "            maxlikelihood_lambda = lambdaVal\n",
    "        likelihoodSave.append(tmp_likelihood)\n",
    "        lambdaValSave.append(lambdaVal)\n",
    "        lambdaVal += stepSize\n",
    "    # Return all of them\n",
    "    return maxlikelihood, maxlikelihood_lambda, likelihoodSave, lambdaValSave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will return covariance matrix\n",
    "def Covariance(bac_tree):\n",
    "    defaultTree = Tree(\"(A:1,(B:1,(C:1,D:1):0.5):0.5);\")\n",
    "    if type(bac_tree) != type(defaultTree):\n",
    "        raise TypeError(f\"bac_tree type should be {type(defaultTree)}\")\n",
    "    # OK\n",
    "    # Create n by n matrix Make sure pruning the tree or it maybe too large and have error occur.\n",
    "    C = np.zeros(shape=(len(bac_tree), len(bac_tree)))\n",
    "    # Used to tranverse through the matrix\n",
    "    i_counter = -1\n",
    "    j_counter = -1\n",
    "    # Tranverse through all leaves\n",
    "    for leaf_i in bac_tree:\n",
    "        # Corresponding index\n",
    "        i_counter += 1\n",
    "        # Tranverse through all leaves\n",
    "        for leaf_j in bac_tree:\n",
    "            j_counter += 1\n",
    "            # If they are the same leaf\n",
    "            if leaf_i == leaf_j:\n",
    "                # Covariance is just its distance to the root\n",
    "                C[i_counter][j_counter] = leaf_i.get_distance(bac_tree)\n",
    "            else:\n",
    "                # Get their first common ancestor and compute its distance to root\n",
    "                commonAncestor = leaf_i.get_common_ancestor(leaf_j)\n",
    "                C[i_counter][j_counter] = commonAncestor.get_distance(bac_tree)\n",
    "        j_counter = -1\n",
    "    return C\n",
    "\n",
    "def traitsColumnReturn(df, traits_name):\n",
    "    traits = list(df.loc[:,f'{traits_name}'])\n",
    "    X = []\n",
    "    for i in traits:\n",
    "        X.append([i])\n",
    "    X = np.array(X)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step size 0.01\n",
    "def Bacteria_Pagel():\n",
    "    # Load trees\n",
    "    # bac_tree is the root of the tree\n",
    "    bac_tree = Tree('./Data/IGG_v1.0_bact_22515.tre')\n",
    "    # Find correspondence\n",
    "    IGG_Haojun = pd.read_csv(\"./Data/igg_haojun.csv\")\n",
    "    IGG_Haojun_altid2genomeid = IGG_Haojun.loc[:, ['genome_id', 'species_alt_id']]\n",
    "\n",
    "    out_all = pd.read_csv(\"./Data/out-all-2.csv\")\n",
    "    out_all_genome_id = out_all.iloc[:,0]\n",
    "    \n",
    "    headers = list(IGG_Haojun_altid2genomeid.columns)\n",
    "    translate = pd.DataFrame(columns=headers)\n",
    "    # find correspondance\n",
    "    for genome_id in out_all_genome_id:\n",
    "        row = IGG_Haojun_altid2genomeid.loc[IGG_Haojun_altid2genomeid['genome_id'] == genome_id]\n",
    "        # translate = translate.append(row,ignore_index=True)\n",
    "        translate = pd.concat([translate, row],ignore_index=True)\n",
    "\n",
    "    # Slice all samples\n",
    "    sample_id = list(out_all.columns)\n",
    "    sample_id = sample_id[1:]\n",
    "    # save for all maximum likelihood lambda in a format of dictionary of dictionary\n",
    "    maximumlikelihoodSave = {}\n",
    "    maximumlikelihoodOtherSave = {}\n",
    "    for i in sample_id:\n",
    "        out_all_new_sample = out_all.loc[:,i]\n",
    "        tmp_translate = translate\n",
    "        out = tmp_translate.join(out_all_new_sample)\n",
    "        keep = out[pd.notnull(out[i])]\n",
    "        keep = keep.reset_index(drop=True)\n",
    "        keep_list = list(keep.iloc[:,1])\n",
    "        # Make the become string\n",
    "        for j in range(0, len(keep_list)):\n",
    "            keep_list[j] = str(keep_list[j])\n",
    "        # Create a tree to do pruning operation\n",
    "        bac_tree_op = bac_tree.copy()\n",
    "        bac_tree_op.prune(keep_list, preserve_branch_length=True)\n",
    "        # Compute covarience\n",
    "        C = Covariance(bac_tree_op)\n",
    "        # Reorder the feature\n",
    "        reorder_header = list(keep.columns)\n",
    "        reorder = pd.DataFrame(columns=reorder_header)\n",
    "        for leaf in bac_tree_op:\n",
    "            row = keep.loc[keep['species_alt_id'] == int(leaf.name)]\n",
    "            # reorder = reorder.append(row,ignore_index=True)\n",
    "            reorder = pd.concat([reorder, row],ignore_index=True)\n",
    "\n",
    "        # Log2(PTR)\n",
    "        X = traitsColumnReturn(reorder, i)\n",
    "        # PTR\n",
    "        X = np.exp2(X)\n",
    "        # Can be negative but be careful with the domain, I do not advise to do so because it is meaningless and may cause math domain error (math.sqrt(negative value))\n",
    "        # Greater than 1 also has math domain error.\n",
    "        # ln maximumlikelihood\n",
    "        # Modified\n",
    "        #maxlikelihood, maxlikelihood_lambda, likelihoodSave, lambdaValSave=Found_Pagel_Maximumlikelihood(X, bac_tree_op, 0.01,startSearch=0,EndSearch=1)\n",
    "        maxlikelihood, maxlikelihood_lambda, likelihoodSave, lambdaValSave=Found_Pagel_Maximumlikelihood(X, bac_tree_op, 0.01,startSearch=-1,EndSearch=1)\n",
    "\n",
    "        if maxlikelihood_lambda != None:\n",
    "            print(f'Sample: {i}; Ln Maximum likelihood: {maxlikelihood}; Number of leaves: {len(keep)}; Lambda: {round(maxlikelihood_lambda,2)}')\n",
    "        else:\n",
    "            print(f'Sample: {i}; Ln Maximum likelihood: {maxlikelihood}; Number of leaves: {len(keep)}; Lambda: {maxlikelihood_lambda}')\n",
    "\n",
    "        if len(keep) not in maximumlikelihoodSave.keys():\n",
    "            maximumlikelihoodSave[len(keep)] = {i:maxlikelihood_lambda}\n",
    "            maximumlikelihoodOtherSave[len(keep)] = {i:[maxlikelihood, likelihoodSave, lambdaValSave]}\n",
    "        else:\n",
    "            maximumlikelihoodSave[len(keep)][i] = maxlikelihood_lambda\n",
    "            maximumlikelihoodOtherSave[len(keep)][i] = [maxlikelihood, likelihoodSave, lambdaValSave]\n",
    "        \n",
    "        # if maxlikelihood_lambda != 0 and maxlikelihood_lambda != None:\n",
    "            # try:\n",
    "        # fig, ax = plt.subplots(1, 1, figsize=(10,10))\n",
    "        plt.plot(lambdaValSave, likelihoodSave, 'ro')\n",
    "        plt.xlabel(\"Lambda Value\")\n",
    "        plt.ylabel(\"Likelihood\")\n",
    "        plt.title(f'Pagel\\'s Lambda: Lambda-Likelihood Plot \\n \\n\\\n",
    "            Sample: {i}; Maximum likelihood: {maxlikelihood}; \\n \\n Number of leaves: {len(keep)}; Lambda: {round(maxlikelihood_lambda,2)}', fontweight='bold', fontsize=12)\n",
    "        # plt.xlim([0, 1]) Modified\n",
    "        plt.xlim([-1, 1])\n",
    "        # ax.savefig(f\"./Plots/{i}%{len(keep)}.png\", bbox_inches = 'tight')\n",
    "        # ax.savefig(f\"./Plots_PTR/{i}%{len(keep)}.png\", bbox_inches = 'tight')\n",
    "        # plt.savefig(f\"./Updated_Plots_PTR_LnMax/{i}%{len(keep)}.png\", bbox_inches = 'tight') Modified\n",
    "        plt.savefig(f\"./Updated_Plots_PTR_LnMax_-1to1/{i}%{len(keep)}.png\", bbox_inches = 'tight')\n",
    "        \n",
    "        plt.clf()\n",
    "            # except Exception as e:\n",
    "            #     print(\"+++++++++++++++++++++++++++++++\")\n",
    "            #     # print(lambdaValSave)\n",
    "            #     # print(likelihoodSave)\n",
    "            #     print(e)\n",
    "            #     print(\"+++++++++++++++++++++++++++++++\")\n",
    "\n",
    "    return maximumlikelihoodSave, maximumlikelihoodOtherSave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximumlikelihoodSave needs a dictionary of dictionary\n",
    "# numBins needs how many bins you want\n",
    "def Lambda_Hist(maximumlikelihoodSave, numBins):\n",
    "    lambdaList = []\n",
    "    for leaf_num in maximumlikelihoodSave.keys():\n",
    "        for sampleVal in maximumlikelihoodSave[leaf_num].keys():\n",
    "            if maximumlikelihoodSave[leaf_num][sampleVal] != None:\n",
    "                lambdaList.append(maximumlikelihoodSave[leaf_num][sampleVal])\n",
    "    plt.hist(lambdaList, numBins)\n",
    "    plt.title(f'Histogram of Maximum Likelihood Lambda \\n \\n Number of Valid Samples: {len(lambdaList)}', \\\n",
    "             fontweight='bold', fontsize=12)\n",
    "    plt.xlabel(\"Lambda Values\")\n",
    "    plt.ylabel(\"Number of Samples\")\n",
    "    # plt.savefig(f\"./Plots/Lambda_Samples_Histogram.png\", bbox_inches = 'tight')\n",
    "    # plt.savefig(f\"./Plots_PTR/Lambda_Samples_Histogram.png\", bbox_inches = 'tight')\n",
    "    #plt.savefig(f\"./Updated_Plots_PTR_LnMax/Lambda_Samples_Histogram.png\", bbox_inches = 'tight') Modified\n",
    "    plt.savefig(f\"./Updated_Plots_PTR_LnMax_-1to1/Lambda_Samples_Histogram.png\", bbox_inches = 'tight')\n",
    "    plt.clf()\n",
    "    #plt.show()\n",
    "\n",
    "def leaves_lambda(maximumlikelihoodSave):\n",
    "    leaves = []\n",
    "    lambdas = []\n",
    "    # {leaves: {sample:lambda}}\n",
    "    for i in maximumlikelihoodSave.keys():\n",
    "        for j in maximumlikelihoodSave[i].keys():\n",
    "            if maximumlikelihoodSave[i][j] != None:\n",
    "                leaves.append(i)\n",
    "                lambdas.append(maximumlikelihoodSave[i][j])\n",
    "    plt.plot(leaves, lambdas, 'ro', alpha=0.5)\n",
    "    plt.xlabel(\"Number of Leaves\")\n",
    "    plt.ylabel(\"Lambda Values\")\n",
    "    plt.title(f\"Pagel\\'s Lambda -- Leaves-Lambda Distribution Plots \\n \\n Number of Samples: {len(lambdas)}\")\n",
    "    # plt.savefig(f\"./Plots/Leaves_lambdas.png\", bbox_inches = 'tight')\n",
    "    # plt.savefig(f\"./Plots_PTR/Leaves_lambdas.png\", bbox_inches = 'tight')\n",
    "    # plt.savefig(f\"./Updated_Plots_PTR_LnMax/Leaves_lambdas.png\", bbox_inches = 'tight') Modified\n",
    "    plt.savefig(f\"./Updated_Plots_PTR_LnMax_-1to1/Leaves_lambdas.png\", bbox_inches = 'tight')\n",
    "    \n",
    "    plt.clf()\n",
    "    #plt.show()\n",
    "\n",
    "def lambda_subplot(maximumlikelihoodSave, maximumlikelihoodOtherSave, width = 0.1, height = 0.1 , opacity = 0.5):\n",
    "    leaves = []\n",
    "    lambdas = []\n",
    "    # {leaves: {sample:lambda}}\n",
    "    for i in maximumlikelihoodSave.keys():\n",
    "        for j in maximumlikelihoodSave[i].keys():\n",
    "            if maximumlikelihoodSave[i][j] != None:\n",
    "                leaves.append(i)\n",
    "                lambdas.append(maximumlikelihoodSave[i][j])\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(leaves, lambdas, 'ro')\n",
    "    ax.set_xlabel(\"Number of Leaves\")\n",
    "    ax.set_ylabel(\"Lambda Values\")\n",
    "    ax.set_title(f\"Pagel\\'s Lambda -- Leaves-Lambda Distribution Plots \\n \\n Number of Samples: {len(lambdas)}\")\n",
    "    # maximumlikelihoodOtherSave[len(keep)] = {i:[maxlikelihood, likelihoodSave, lambdaValSave]}\n",
    "    for i in maximumlikelihoodOtherSave.keys():\n",
    "        for j in maximumlikelihoodOtherSave[i].keys():\n",
    "            if maximumlikelihoodSave[i][j] != None:\n",
    "                x_trans = (i - ax.get_xlim()[0]) / (ax.get_xlim()[1] - ax.get_xlim()[0])\n",
    "                y_trans = (maximumlikelihoodSave[i][j] - ax.get_ylim()[0])/ (ax.get_ylim()[1] - ax.get_ylim()[0])\n",
    "                axins = ax.inset_axes([x_trans, y_trans, width, height])\n",
    "                axins.plot(maximumlikelihoodOtherSave[i][j][2], maximumlikelihoodOtherSave[i][j][1])\n",
    "                axins.patch.set_alpha(opacity)\n",
    "    # fig.savefig(f\"./Updated_Plots_PTR_LnMax/LeavesLambdaDistribution.png\", bbox_inches = 'tight') Modified\n",
    "    fig.savefig(f\"./Updated_Plots_PTR_LnMax_-1to1/LeavesLambdaDistribution.png\", bbox_inches = 'tight')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "# maximumlikelihoodSave, maximumlikelihoodOtherSave = Bacteria_Pagel()\n",
    "# Lambda_Hist(maximumlikelihoodSave, 10)\n",
    "# leaves_lambda(maximumlikelihoodSave)\n",
    "# lambda_subplot(maximumlikelihoodSave, maximumlikelihoodOtherSave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''The column names have the format \"{patient_id}_K{visit_num}0\", i.e. \"EP003595_K10\" means \"patient EP003595\" visit 1. \n",
    "The row names are Greengenes IDs, and they should map to the Greengenes 12_10 tree I showed you. \n",
    "I'm not sure what the actual time intervals are. You can use subsequent visits for the same patient to look at general growth rates.'''\n",
    "def MOMS_PI_Dataset_Pagel():\n",
    "    # Process data first\n",
    "    with open(\"C:/Users/12533/Desktop/Fall 2022/COMSE 6901/Fall 2022/MOMS-PI dataset/mixture/16s/16s_tables.pkl\", \"rb\") as f:\n",
    "        tables = pickle.load(f)\n",
    "    # Top 10% densest clades\n",
    "    top10_clades = (tables['MCKD'].to_dataframe() > 0).sum(axis=1).sort_values()[-375:]\n",
    "    # Normalization: make every sample sum to 1\n",
    "    mckd_normed = tables['MCKD'].norm(axis='sample', inplace=False)\n",
    "    mckd_normed_top10 = mckd_normed.filter(top10_clades.index, axis='observation', inplace=False) # Filter to top 10% densest clades\n",
    "    \n",
    "    colnames_split = [x.split(\"_\") for x in mckd_normed_top10.to_dataframe().columns]\n",
    "    colnames_split2 = [(patient, int(visit[1:-1])) for (patient, visit) in colnames_split]\n",
    "    mckd_normed_top10_reindexed = mckd_normed_top10.copy().to_dataframe()\n",
    "    mckd_normed_top10_reindexed.columns = pd.MultiIndex.from_tuples(colnames_split2) # Make a multiindex with patient and visit\n",
    "    # Sort by patient and visit\n",
    "    mckd_normed_top10_reindexed = mckd_normed_top10_reindexed.sort_index(axis=1, level=1) # Sort by visit\n",
    "    mckd_normed_top10_reindexed = mckd_normed_top10_reindexed.sort_index(axis=1, level=0) # Sort by patient\n",
    "    \n",
    "    mckd_diffs = mckd_normed_top10_reindexed.groupby(level=0, axis=1).diff() # Group by patient, then take difference between visits\n",
    "    \n",
    "    # Drop the columns where all elements are NaN:\n",
    "    mckd_diffs = mckd_diffs.dropna(axis=1, how='all')\n",
    "    # OLD CODE Modifiy\n",
    "    \n",
    "    # Load trees\n",
    "    # bac_tree is the root of the tree\n",
    "    bac_tree = Tree('C:/Users/12533/Desktop/Fall 2022/COMSE 6901/Fall 2022/MOMS-PI dataset/gg_13_5_otus_99_annotated_Newick.tree')\n",
    "\n",
    "    # save for all maximum likelihood lambda in a format of dictionary of dictionary\n",
    "    maximumlikelihoodSave = {}\n",
    "    maximumlikelihoodOtherSave = {}\n",
    "    \n",
    "    for patientID, visitTimes in mckd_diffs.columns.tolist():\n",
    "        keep = pd.DataFrame(mckd_diffs[patientID][visitTimes])\n",
    "        keep_list = []\n",
    "        for i in keep.index:\n",
    "            keep_list.append(str(i))\n",
    "        # print(keep_list)\n",
    "        # return\n",
    "        # Create a tree to do pruning operation\n",
    "        bac_tree_op = bac_tree.copy()\n",
    "        bac_tree_op.prune(keep_list, preserve_branch_length=True)\n",
    "\n",
    "        # Reorder the feature\n",
    "        reorder = pd.DataFrame()\n",
    "        for leaf in bac_tree_op:\n",
    "            row = keep.loc[str(leaf.name)]\n",
    "            # reorder = reorder.append(row,ignore_index=True)\n",
    "            reorder = pd.concat([reorder, row],ignore_index=True)\n",
    "\n",
    "        # print(reorder)\n",
    "        X = reorder.iloc[:, 0].to_numpy()\n",
    "        X = np.array([X])\n",
    "        X = X.T\n",
    "        # Can be negative but be careful with the domain, I do not advise to do so because it is meaningless and may cause math domain error (math.sqrt(negative value))\n",
    "        # Greater than 1 also has math domain error.\n",
    "        # ln maximumlikelihood\n",
    "        # Modified\n",
    "        maxlikelihood, maxlikelihood_lambda, likelihoodSave, lambdaValSave=Found_Pagel_Maximumlikelihood(X, bac_tree_op, 0.01,startSearch=0,EndSearch=1)\n",
    "        # {patient_id}_K{visit_num}0\n",
    "        if maxlikelihood_lambda != None:\n",
    "            print(f'Sample: {patientID}_K{visitTimes}0; Ln Maximum likelihood: {maxlikelihood}; Number of leaves: {len(keep.index)}; Lambda: {round(maxlikelihood_lambda, 2)}')\n",
    "        else:\n",
    "            print(f'Sample: {patientID}_K{visitTimes}0; Ln Maximum likelihood: {maxlikelihood}; Number of leaves: {len(keep.index)}; Lambda: {maxlikelihood_lambda}')\n",
    "\n",
    "        if len(keep.index) not in maximumlikelihoodSave.keys():\n",
    "            maximumlikelihoodSave[len(keep.index)] = {f'{patientID}_K{visitTimes}0':maxlikelihood_lambda}\n",
    "            maximumlikelihoodOtherSave[len(keep.index)] = {f'{patientID}_K{visitTimes}0':[maxlikelihood, likelihoodSave, lambdaValSave]}\n",
    "        else:\n",
    "            maximumlikelihoodSave[len(keep.index)][f'{patientID}_K{visitTimes}0'] = maxlikelihood_lambda\n",
    "            maximumlikelihoodOtherSave[len(keep.index)][f'{patientID}_K{visitTimes}0'] = [maxlikelihood, likelihoodSave, lambdaValSave]\n",
    "        \n",
    "\n",
    "        plt.plot(lambdaValSave, likelihoodSave, 'ro')\n",
    "        plt.xlabel(\"Lambda Value\")\n",
    "        plt.ylabel(\"Likelihood\")\n",
    "        plt.title(f'Pagel\\'s Lambda: Lambda-Likelihood Plot \\n \\n\\\n",
    "            Sample: {patientID}_K{visitTimes}0; Ln Maximum likelihood: {maxlikelihood}; \\n \\n Number of leaves: {len(keep.index)}; \\\n",
    "                Lambda: {round(maxlikelihood_lambda,2)}', fontweight='bold', fontsize=12)\n",
    "        plt.xlim([0, 1])\n",
    "        plt.savefig(f\"../Plots_MOMS-PI_LnMax/{patientID}_K{visitTimes}0%{len(keep.index)}.png\", bbox_inches = 'tight')\n",
    "        \n",
    "        plt.clf()\n",
    "    return maximumlikelihoodSave, maximumlikelihoodOtherSave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\12533\\AppData\\Local\\Temp\\ipykernel_10580\\946235080.py:7: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  tables = pickle.load(f)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m MOMS_PI_Dataset_Pagel()\n",
      "Cell \u001b[1;32mIn [12], line 22\u001b[0m, in \u001b[0;36mMOMS_PI_Dataset_Pagel\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m mckd_normed_top10_reindexed \u001b[39m=\u001b[39m mckd_normed_top10_reindexed\u001b[39m.\u001b[39msort_index(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, level\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m# Sort by visit\u001b[39;00m\n\u001b[0;32m     20\u001b[0m mckd_normed_top10_reindexed \u001b[39m=\u001b[39m mckd_normed_top10_reindexed\u001b[39m.\u001b[39msort_index(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, level\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m) \u001b[39m# Sort by patient\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m mckd_diffs \u001b[39m=\u001b[39m mckd_normed_top10_reindexed\u001b[39m.\u001b[39;49mgroupby(level\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39;49mdiff() \u001b[39m# Group by patient, then take difference between visits\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[39m# Drop the columns where all elements are NaN:\u001b[39;00m\n\u001b[0;32m     25\u001b[0m mckd_diffs \u001b[39m=\u001b[39m mckd_diffs\u001b[39m.\u001b[39mdropna(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, how\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mall\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\12533\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:768\u001b[0m, in \u001b[0;36m_GroupBy._make_wrapper.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply(curried)\n\u001b[0;32m    767\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 768\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_python_apply_general(curried, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_obj_with_exclusions)\n\u001b[0;32m    769\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    770\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m re\u001b[39m.\u001b[39msearch(\n\u001b[0;32m    771\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mreduction operation \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.*\u001b[39m\u001b[39m'\u001b[39m\u001b[39m not allowed for this dtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mstr\u001b[39m(err)\n\u001b[0;32m    772\u001b[0m     ):\n\u001b[0;32m    773\u001b[0m         \u001b[39m# We don't have a cython implementation\u001b[39;00m\n\u001b[0;32m    774\u001b[0m         \u001b[39m# TODO: is the above comment accurate?\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\12533\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:892\u001b[0m, in \u001b[0;36m_GroupBy._python_apply_general\u001b[1;34m(self, f, data)\u001b[0m\n\u001b[0;32m    874\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_python_apply_general\u001b[39m(\n\u001b[0;32m    875\u001b[0m     \u001b[39mself\u001b[39m, f: F, data: FrameOrSeriesUnion\n\u001b[0;32m    876\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m FrameOrSeriesUnion:\n\u001b[0;32m    877\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    878\u001b[0m \u001b[39m    Apply function f in python space\u001b[39;00m\n\u001b[0;32m    879\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    890\u001b[0m \u001b[39m        data after applying f\u001b[39;00m\n\u001b[0;32m    891\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 892\u001b[0m     keys, values, mutated \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgrouper\u001b[39m.\u001b[39;49mapply(f, data, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maxis)\n\u001b[0;32m    894\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wrap_applied_output(\n\u001b[0;32m    895\u001b[0m         keys, values, not_indexed_same\u001b[39m=\u001b[39mmutated \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmutated\n\u001b[0;32m    896\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\12533\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\ops.py:213\u001b[0m, in \u001b[0;36mBaseGrouper.apply\u001b[1;34m(self, f, data, axis)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[39m# group might be modified\u001b[39;00m\n\u001b[0;32m    212\u001b[0m group_axes \u001b[39m=\u001b[39m group\u001b[39m.\u001b[39maxes\n\u001b[1;32m--> 213\u001b[0m res \u001b[39m=\u001b[39m f(group)\n\u001b[0;32m    214\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_indexed_like(res, group_axes):\n\u001b[0;32m    215\u001b[0m     mutated \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\12533\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:756\u001b[0m, in \u001b[0;36m_GroupBy._make_wrapper.<locals>.wrapper.<locals>.curried\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcurried\u001b[39m(x):\n\u001b[1;32m--> 756\u001b[0m     \u001b[39mreturn\u001b[39;00m f(x, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\12533\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:7254\u001b[0m, in \u001b[0;36mDataFrame.diff\u001b[1;34m(self, periods, axis)\u001b[0m\n\u001b[0;32m   7251\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_consolidate_inplace()\n\u001b[0;32m   7253\u001b[0m \u001b[39mif\u001b[39;00m bm_axis \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m periods \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m-> 7254\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mT\u001b[39m.\u001b[39;49mdiff(periods, axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39mT\n\u001b[0;32m   7256\u001b[0m new_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mgr\u001b[39m.\u001b[39mdiff(n\u001b[39m=\u001b[39mperiods, axis\u001b[39m=\u001b[39mbm_axis)\n\u001b[0;32m   7257\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_constructor(new_data)\n",
      "File \u001b[1;32mc:\\Users\\12533\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:7256\u001b[0m, in \u001b[0;36mDataFrame.diff\u001b[1;34m(self, periods, axis)\u001b[0m\n\u001b[0;32m   7253\u001b[0m \u001b[39mif\u001b[39;00m bm_axis \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m periods \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   7254\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mT\u001b[39m.\u001b[39mdiff(periods, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mT\n\u001b[1;32m-> 7256\u001b[0m new_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mgr\u001b[39m.\u001b[39;49mdiff(n\u001b[39m=\u001b[39;49mperiods, axis\u001b[39m=\u001b[39;49mbm_axis)\n\u001b[0;32m   7257\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_constructor(new_data)\n",
      "File \u001b[1;32mc:\\Users\\12533\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py:558\u001b[0m, in \u001b[0;36mBlockManager.diff\u001b[1;34m(self, n, axis)\u001b[0m\n\u001b[0;32m    557\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdiff\u001b[39m(\u001b[39mself\u001b[39m, n: \u001b[39mint\u001b[39m, axis: \u001b[39mint\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mBlockManager\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 558\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply(\u001b[39m\"\u001b[39;49m\u001b[39mdiff\u001b[39;49m\u001b[39m\"\u001b[39;49m, n\u001b[39m=\u001b[39;49mn, axis\u001b[39m=\u001b[39;49maxis)\n",
      "File \u001b[1;32mc:\\Users\\12533\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py:406\u001b[0m, in \u001b[0;36mBlockManager.apply\u001b[1;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[0;32m    404\u001b[0m         applied \u001b[39m=\u001b[39m b\u001b[39m.\u001b[39mapply(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    405\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 406\u001b[0m         applied \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(b, f)(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    407\u001b[0m     result_blocks \u001b[39m=\u001b[39m _extend_blocks(applied, result_blocks)\n\u001b[0;32m    409\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(result_blocks) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\12533\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:1807\u001b[0m, in \u001b[0;36mExtensionBlock.diff\u001b[1;34m(self, n, axis)\u001b[0m\n\u001b[0;32m   1803\u001b[0m \u001b[39mif\u001b[39;00m axis \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1804\u001b[0m     \u001b[39m# TODO(EA2D): unnecessary with 2D EAs\u001b[39;00m\n\u001b[0;32m   1805\u001b[0m     \u001b[39m# we are by definition 1D.\u001b[39;00m\n\u001b[0;32m   1806\u001b[0m     axis \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m-> 1807\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mdiff(n, axis)\n",
      "File \u001b[1;32mc:\\Users\\12533\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:1272\u001b[0m, in \u001b[0;36mBlock.diff\u001b[1;34m(self, n, axis)\u001b[0m\n\u001b[0;32m   1270\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdiff\u001b[39m(\u001b[39mself\u001b[39m, n: \u001b[39mint\u001b[39m, axis: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39m\"\u001b[39m\u001b[39mBlock\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m   1271\u001b[0m     \u001b[39m\"\"\" return block for the diff of the values \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1272\u001b[0m     new_values \u001b[39m=\u001b[39m algos\u001b[39m.\u001b[39;49mdiff(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalues, n, axis\u001b[39m=\u001b[39;49maxis, stacklevel\u001b[39m=\u001b[39;49m\u001b[39m7\u001b[39;49m)\n\u001b[0;32m   1273\u001b[0m     \u001b[39mreturn\u001b[39;00m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_block(values\u001b[39m=\u001b[39mnew_values)]\n",
      "File \u001b[1;32mc:\\Users\\12533\\anaconda3\\lib\\site-packages\\pandas\\core\\algorithms.py:1910\u001b[0m, in \u001b[0;36mdiff\u001b[1;34m(arr, n, axis, stacklevel)\u001b[0m\n\u001b[0;32m   1908\u001b[0m \u001b[39mif\u001b[39;00m is_extension_array_dtype(dtype):\n\u001b[0;32m   1909\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(arr, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m__\u001b[39m\u001b[39m{\u001b[39;00mop\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m__\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m-> 1910\u001b[0m         \u001b[39mreturn\u001b[39;00m op(arr, arr\u001b[39m.\u001b[39;49mshift(n))\n\u001b[0;32m   1911\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1912\u001b[0m         warn(\n\u001b[0;32m   1913\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mdtype lost in \u001b[39m\u001b[39m'\u001b[39m\u001b[39mdiff()\u001b[39m\u001b[39m'\u001b[39m\u001b[39m. In the future this will raise a \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1914\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mTypeError. Convert to a suitable dtype prior to calling \u001b[39m\u001b[39m'\u001b[39m\u001b[39mdiff\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1915\u001b[0m             \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m   1916\u001b[0m             stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[0;32m   1917\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\12533\\anaconda3\\lib\\site-packages\\pandas\\core\\ops\\common.py:65\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[39mreturn\u001b[39;00m \u001b[39mNotImplemented\u001b[39m\n\u001b[0;32m     63\u001b[0m other \u001b[39m=\u001b[39m item_from_zerodim(other)\n\u001b[1;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m method(\u001b[39mself\u001b[39;49m, other)\n",
      "File \u001b[1;32mc:\\Users\\12533\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\sparse\\array.py:1407\u001b[0m, in \u001b[0;36mSparseArray._create_arithmetic_method.<locals>.sparse_arithmetic_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m   1403\u001b[0m \u001b[39m@unpack_zerodim_and_defer\u001b[39m(op_name)\n\u001b[0;32m   1404\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msparse_arithmetic_method\u001b[39m(\u001b[39mself\u001b[39m, other):\n\u001b[0;32m   1406\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(other, SparseArray):\n\u001b[1;32m-> 1407\u001b[0m         \u001b[39mreturn\u001b[39;00m _sparse_array_op(\u001b[39mself\u001b[39;49m, other, op, op_name)\n\u001b[0;32m   1409\u001b[0m     \u001b[39melif\u001b[39;00m is_scalar(other):\n\u001b[0;32m   1410\u001b[0m         \u001b[39mwith\u001b[39;00m np\u001b[39m.\u001b[39merrstate(\u001b[39mall\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\12533\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\sparse\\array.py:162\u001b[0m, in \u001b[0;36m_sparse_array_op\u001b[1;34m(left, right, op, name)\u001b[0m\n\u001b[0;32m    159\u001b[0m     sparse_op \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(splib, opname)\n\u001b[0;32m    161\u001b[0m     \u001b[39mwith\u001b[39;00m np\u001b[39m.\u001b[39merrstate(\u001b[39mall\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 162\u001b[0m         result, index, fill \u001b[39m=\u001b[39m sparse_op(\n\u001b[0;32m    163\u001b[0m             left_sp_values,\n\u001b[0;32m    164\u001b[0m             left\u001b[39m.\u001b[39;49msp_index,\n\u001b[0;32m    165\u001b[0m             left\u001b[39m.\u001b[39;49mfill_value,\n\u001b[0;32m    166\u001b[0m             right_sp_values,\n\u001b[0;32m    167\u001b[0m             right\u001b[39m.\u001b[39;49msp_index,\n\u001b[0;32m    168\u001b[0m             right\u001b[39m.\u001b[39;49mfill_value,\n\u001b[0;32m    169\u001b[0m         )\n\u001b[0;32m    171\u001b[0m \u001b[39mif\u001b[39;00m result_dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    172\u001b[0m     result_dtype \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39mdtype\n",
      "File \u001b[1;32mpandas\\_libs\\sparse_op_helper.pxi:586\u001b[0m, in \u001b[0;36mpandas._libs.sparse.sparse_sub_float64\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\sparse_op_helper.pxi:595\u001b[0m, in \u001b[0;36mpandas._libs.sparse.sparse_sub_float64\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\sparse_op_helper.pxi:545\u001b[0m, in \u001b[0;36mpandas._libs.sparse.int_op_sub_float64\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\sparse.pyx:175\u001b[0m, in \u001b[0;36mpandas._libs.sparse.IntIndex.make_union\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36munion1d\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\12533\\anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:781\u001b[0m, in \u001b[0;36munion1d\u001b[1;34m(ar1, ar2)\u001b[0m\n\u001b[0;32m    747\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_union1d_dispatcher)\n\u001b[0;32m    748\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39munion1d\u001b[39m(ar1, ar2):\n\u001b[0;32m    749\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    750\u001b[0m \u001b[39m    Find the union of two arrays.\u001b[39;00m\n\u001b[0;32m    751\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    779\u001b[0m \u001b[39m    array([1, 2, 3, 4, 6])\u001b[39;00m\n\u001b[0;32m    780\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 781\u001b[0m     \u001b[39mreturn\u001b[39;00m unique(np\u001b[39m.\u001b[39;49mconcatenate((ar1, ar2), axis\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m))\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36munique\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\12533\\anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:274\u001b[0m, in \u001b[0;36munique\u001b[1;34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001b[0m\n\u001b[0;32m    272\u001b[0m ar \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masanyarray(ar)\n\u001b[0;32m    273\u001b[0m \u001b[39mif\u001b[39;00m axis \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 274\u001b[0m     ret \u001b[39m=\u001b[39m _unique1d(ar, return_index, return_inverse, return_counts, \n\u001b[0;32m    275\u001b[0m                     equal_nan\u001b[39m=\u001b[39;49mequal_nan)\n\u001b[0;32m    276\u001b[0m     \u001b[39mreturn\u001b[39;00m _unpack_tuple(ret)\n\u001b[0;32m    278\u001b[0m \u001b[39m# axis was specified and not None\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\12533\\anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:328\u001b[0m, in \u001b[0;36m_unique1d\u001b[1;34m(ar, return_index, return_inverse, return_counts, equal_nan)\u001b[0m\n\u001b[0;32m    323\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_unique1d\u001b[39m(ar, return_index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, return_inverse\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    324\u001b[0m               return_counts\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m, equal_nan\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    325\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    326\u001b[0m \u001b[39m    Find the unique elements of an array, ignoring shape.\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 328\u001b[0m     ar \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49masanyarray(ar)\u001b[39m.\u001b[39;49mflatten()\n\u001b[0;32m    330\u001b[0m     optional_indices \u001b[39m=\u001b[39m return_index \u001b[39mor\u001b[39;00m return_inverse\n\u001b[0;32m    332\u001b[0m     \u001b[39mif\u001b[39;00m optional_indices:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "MOMS_PI_Dataset_Pagel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ff334016b38fbb0d8537ec08ac793e44ef7bf4289686337bb93b6dafc2584592"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
